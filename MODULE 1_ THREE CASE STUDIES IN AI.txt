MODULE 1: THREE CASE STUDIES IN AI
Practical Applications of AI Concepts
Total Time: 30 minutes (3 case studies × 12 minutes each)
________________


CASE STUDY 1: Netflix Recommendation System
"Why Does Netflix Know What You Want to Watch?"
Time Allocation: 12 minutes | Reading: 2-3 min | Explanation: 3-5 min | Discussion: 5-10 min
Scenario Summary (2-3 minutes reading)
The Situation: Maria, a Netflix subscriber in Miami, opens the app on her Tuesday evening. Rather than browsing thousands of titles, she's immediately greeted with personalized recommendations: "Because you watched The Office, you might like Parks and Recreation." Below that: "Popular in Drama." Another section: "Because you rated Stranger Things 5 stars, check out The Witcher."
Maria is amazed. She hasn't told Netflix explicitly what she likes. She hasn't filled out a preference form. Yet the platform understands her tastes remarkably well. How?
The Data Behind Maria's Experience: Netflix collects vast amounts of behavioral data:
* Every show and movie she watches (complete vs. partial viewing)
* When she pauses, rewinds, or fast-forwards
* What she rates (thumbs up/down)
* Time of day she watches
* Device she uses
* Genre preferences inferred from patterns
* Similar data from millions of other subscribers
Netflix doesn't sell content to Maria. It sells attention to content producers. Better recommendations = more watch time = more valuable to advertisers and producers. The company has invested billions in perfecting its recommendation engine.
The Business Impact:
* Subscribers who receive personalized recommendations watch 37% more content
* Retention rates improve dramatically with good recommendations
* Netflix spends roughly $1 billion annually on recommendation algorithms
* The company credits recommendations with preventing customer churn (preventing cancellations)
________________


AI Concept Explanation (3-5 minutes)
How This Is "Machine Learning" Not "Artificial Intelligence"
Netflix's recommendation system is a perfect example of Machine Learning (Aprendizaje Automático) rather than general "Artificial Intelligence."
Why It's Machine Learning:
1. Learning from Data: The system doesn't follow explicit rules programmed by Netflix engineers. Instead, it learns patterns from millions of user interactions.
2. Supervised Learning: Netflix provides labeled examples—users rating content explicitly (thumbs up/down) or implicitly (watching to completion vs. abandoning). The system learns to predict these patterns.
3. Pattern Recognition: The algorithm discovers that users who watch crime dramas often enjoy procedural shows, or that people who watch British comedies tend to enjoy wit-heavy content.
Type of Machine Learning: Collaborative Filtering
Netflix uses "collaborative filtering," which operates on a simple principle: People with similar past viewing habits will likely enjoy similar future content.
How It Works (Simplified):
Step 1: Netflix creates a massive matrix
        User 1  User 2  User 3  User 4  Maria
Movie A   5       5       3       ?       5
Movie B   3       4       2       4       ?
Movie C   1       2       4       5       1


Step 2: Algorithm finds users similar to Maria
        (Users 1 and 2 have very similar preferences)


Step 3: Algorithm checks what Users 1 and 2 rated that Maria hasn't watched
        (Both rated Movie B highly; Maria hasn't seen it)


Step 4: System recommends Movie B to Maria
        ("Because users similar to you liked this...")


The Machine Learning Element:
Rather than Netflix engineers coding "IF user watches crime drama THEN recommend police procedural," the system autonomously learns these associations from data. When viewing habits shift (everyone starts watching more sci-fi), the system adapts automatically.
Why This Isn't "Deep Learning" (Yet)
Netflix's core recommendation engine is sophisticated Machine Learning, but not necessarily Deep Learning (Aprendizaje Profundo).
Deep Learning uses neural networks with multiple layers, which excels at:
* Complex pattern recognition in unstructured data (images, audio)
* Processing sequential information (text, video)
* Learning hierarchical features
Recommendation systems can use deep learning (Netflix experimented with neural networks), but simpler ML techniques often work just as well while being more transparent and requiring less computational power.
Netflix's Evolution:
* Early system: Simple collaborative filtering (pure ML, not DL)
* Modern system: Hybrid approach combining collaborative filtering, content-based filtering (analyzing plot, genre, cast), and deep learning on user behavior sequences
________________


Discussion and Reflection Questions (5-10 minutes)
Core Understanding Questions:
1. Why is Netflix's recommendation system an example of Machine Learning rather than a programmer writing explicit rules?
   * What would explicit rules look like? (Answer: "If user watched 3+ crime shows, recommend crime shows" - but this is too rigid)
   * How does the system learn differently? (Answer: It discovers patterns from data that humans might miss, like "users who watch Danish TV shows also watch Nordic noir")
2. What data does Netflix use to make recommendations, and where does this data come from?
   * List types: Viewing history, ratings, time watched, device type, time of day, pause/rewind patterns, search queries
   * Question: Is this data collection ethical? What are the tradeoffs?
3. Netflix uses "collaborative filtering." In plain language, what principle does this approach rely on?
   * Answer: People with similar tastes in the past will likely have similar tastes in the future
   * Critical thinking: Is this always true? When might it fail? (Answer: When people's tastes change dramatically, or when they're exploring new genres)
Analysis Questions:
4. What type of Machine Learning does Netflix's recommendation system use: Supervised, Unsupervised, or Reinforcement?
   * Answer: Primarily Supervised Learning
   * Why? Netflix provides labeled data (ratings). The system learns to predict ratings based on user features.
   * Secondary element: Reinforcement Learning Netflix tests recommendations and learns which ones keep users engaged longer (reward signal)
5. What are the limitations of Netflix's collaborative filtering approach?
   * New User Problem: A brand-new user with no viewing history gets poor recommendations (can't find similar users)
   * New Content Problem: A newly released show can't be recommended until users watch and rate it
   * Minority Taste Problem: Users with unusual preferences get poor recommendations (fewer similar users to compare against)
   * Popularity Bias: System tends to recommend popular content (more user data available) vs. obscure gems
Ethical Discussion:
6. What are the ethical implications of Netflix's recommendation system?
   * Positive: Users discover content they genuinely enjoy; better experience
   * Concerns:
      * Surveillance: Netflix collects extensive behavioral data. Users might not fully consent or understand scope
      * Filter Bubbles: If the system only recommends similar content, users may miss diverse perspectives. Someone who watches only political content from one side might see only that perspective reinforced
      * Manipulation: Could Netflix optimize recommendations for profit (promoting expensive content or ads) rather than user satisfaction?
      * Data Security: What if this data is breached? Viewing habits reveal sensitive information about people
7. If you were Maria, how comfortable would you be with Netflix collecting all this data about your viewing habits?
   * Consider: What information does Netflix actually have about you?
   * Consider: What could someone infer about you from your Netflix history?
   * Consider: Is the convenience worth the privacy tradeoff?
Real-World Application:
8. Where else do you encounter similar recommendation systems in your daily life?
   * YouTube recommendations
   * Amazon "Customers who bought X also bought Y"
   * Spotify music recommendations
   * Social media feeds (Facebook, Instagram, TikTok)
   * Dating apps (matching people with similar profiles)
   * News feeds (showing articles based on past reading)
9. How could these systems go wrong?
   * Examples of recommendation system failures:
      * YouTube's algorithm once recommended increasingly extreme political content, radicalizing users
      * Spotify's Discover Weekly sometimes recommends artists based on algorithmic associations that seem bizarre
      * Amazon recommendations sometimes suggest products at inflated prices
      * Dating apps sometimes filter out demographic groups, embedding discrimination
________________
















CASE STUDY 2: Facial Recognition in Law Enforcement
"Can AI Reliably Identify Criminal Suspects?"
Time Allocation: 12 minutes | Reading: 2-3 min | Explanation: 3-5 min | Discussion: 5-10 min
Scenario Summary (2-3 minutes reading)
The Situation: Police in Orlando receive a report of a jewelry store robbery. Security cameras capture footage of the suspect. Rather than manually reviewing thousands of mugshots, a detective uploads the image into a facial recognition system.
Within seconds, the AI suggests three potential matches from a database of 6 million mugshots. One match shows 94% confidence: "Likely match to James Rodriguez, prior robbery conviction."
Police use this lead to locate and arrest Rodriguez. Later, DNA evidence confirms Rodriguez was at the scene. Case solved efficiently.
But this happened 15 times that month—and in two cases, the AI's top match was actually innocent. One innocent man was arrested, detained for 36 hours, and released only when DNA didn't match. Another man matched the algorithm's profile but wasn't at the scene; his alibi (security camera footage from a store) proved his innocence.
The Scale of the Problem:
* The FBI's facial recognition database contains millions of images (drivers' licenses, mugshots, passport photos)
* Studies show these systems have higher error rates for women and people with darker skin tones
* Departments use these systems thousands of times daily for investigative leads
* Many innocent people might face scrutiny based on algorithmic suggestions
* The question: Is this "AI working" or "AI making mistakes"?
________________


AI Concept Explanation (3-5 minutes)
This Is "Narrow AI" + "Deep Learning"
Police facial recognition is an example of Narrow AI (IA Estrecha) powered by Deep Learning (Aprendizaje Profundo).
Why It's Narrow AI:
The system does one specific task: compare a face in an image to a database of faces. It cannot:
* Understand context ("This person looks suspicious")
* Reason about likelihood ("Does this person actually live in Orlando?")
* Transfer learning to other tasks (recognizing voices, analyzing documents)
* Explain its reasoning ("I recognize this nose shape and eye distance from...")
This is specialized intelligence, not general intelligence.
Why Deep Learning Is Involved:
Facial recognition uses Convolutional Neural Networks (CNNs), a type of deep learning.
How Facial Recognition Works (Simplified):
Step 1: Input Image
        Computer sees pixel values (color, brightness at each position)


Step 2: Early Layers Extract Simple Features
        Layer 1: Detects edges (face boundary, cheekbone outline)
        Layer 2: Combines edges into shapes (eye shape, nose profile)
        Layer 3: Recognizes features (specific eye color, mouth shape)


Step 3: Middle Layers Learn Identity Patterns
        Layer 4-6: Learns that certain feature combinations = specific identity
        Creates a mathematical "fingerprint" unique to each face


Step 4: Comparison
        Convert suspect image into fingerprint vector
        Compare to database of fingerprint vectors
        Return closest matches (highest similarity scores)


Step 5: Output
        "94% match to Rodriguez"
        "87% match to Garcia"
        "81% match to Smith"


The Learning Process:
Facial recognition systems are trained on millions of labeled faces:
* Each photo is labeled with the person's identity
* The neural network learns to transform faces into numerical vectors such that similar faces have similar vectors
* The system learns through supervised learning—given thousands of examples and correct answers
The Bias Problem:
Research shows facial recognition systems exhibit significant bias:
* Race/Ethnicity Bias: Error rates for Black women can be 34% higher than for white men
* Gender Bias: Misidentify women at higher rates than men
* Age Bias: Perform worse on older adults or children
* Skin Tone Bias: Perform worse on darker skin tones
Why This Happens:
1. Training Data Bias: If training data contains more light-skinned faces, the system learns to recognize those better
2. Historical Data Bias: Mugshot databases over-represent minorities due to policing disparities
3. Technical Factors: Some facial recognition cameras perform worse in certain lighting, which can disproportionately affect people with darker skin tones
This is Narrow AI + Limited Memory:
The system uses only the current image (no memory of prior interactions). It cannot reason about context or apply human judgment.
________________


Discussion and Reflection Questions (5-10 minutes)
Understanding Questions:
1. Why is facial recognition considered a "Deep Learning" application rather than simple Machine Learning?
   * Answer: It requires multiple neural network layers to extract increasingly complex features (edges → shapes → identity patterns)
   * Contrast: Simpler ML might work on pre-extracted facial features, but deep learning learns feature extraction automatically
2. What type of learning does facial recognition use: Supervised, Unsupervised, or Reinforcement?
   * Answer: Supervised Learning
   * Why? The system is trained on millions of labeled images (identity known in advance)
   * How does it learn? Minimizes error between predicted identity and actual identity through backpropagation
3. Explain in your own words why facial recognition might have higher error rates for people with darker skin tones.
   * Possible answers:
      * Training data included more light-skinned faces (data bias)
      * Historical over-representation in criminal databases (societal bias)
      * Camera/lighting factors affecting darker complexions differently (technical bias)
Analysis Questions:
4. The case study mentions facial recognition had 94% confidence but still made errors. What does "94% confidence" actually mean?
   * Important distinction: 94% doesn't mean "94% chance this is correct"
   * Actually means: "This face is 94% similar to Rodriguez's face in the database based on our mathematical vector comparison"
   * Problem: If multiple people are 91-94% similar to the suspect, and the database has millions of people, false matches are likely
   * The base rate problem: Even 99% accuracy sounds good until you realize that with 6 million mugshots, 1% error rate = 60,000 false matches
5. The police arrested an innocent person based on the AI suggestion. Who is responsible for this error?
   * Possible perspectives:
      * The AI system (but it shouldn't be the sole evidence)
      * Police officers (for not independently verifying before arrest)
      * The system designers (for not warning about error rates or bias)
      * The database (for including innocent people's photos or mislabeling)
   * Best answer: Shared responsibility; human judgment must verify AI suggestions
6. Should facial recognition be used as: a) Primary evidence for arrest (sending innocent people to jail if wrong)? b) Investigative lead requiring further evidence (helps narrow suspect list)? c) Not used at all due to bias?
   * Arguments for (b): Efficient investigative tool; massive time savings for police; only used as lead, not conviction evidence
   * Arguments against: Bias against minorities; can trigger unnecessary surveillance; creates presumption
   * Current legal status (in some jurisdictions): Many police departments have policies requiring independent verification; some courts forbid it as primary evidence
Ethical Discussion:
7. What are the ethical concerns with facial recognition in law enforcement?
   * Accuracy & Bias: System performs worse on minorities, potentially leading to discriminatory outcomes
   * Presumption of Innocence: Even investigative leads can bias investigations ("We already think it's you")
   * Privacy: Creates surveillance infrastructure; chilling effects on public assembly
   * Misidentification Risk: Wrong people get arrested, detained, traumatized
   * Systemic Inequality: If training data reflects historical policing bias, system perpetuates it
8. A company claims it has "fixed" the bias in facial recognition by training on diverse faces. Should we trust this?
   * Skepticism warranted: "Fixed" bias is extremely difficult
   * Why? Different lighting, angles, makeup, aging affect recognition differently across groups
   * Better approach: Transparency about error rates broken down by demographic group; independent auditing; human oversight
Real-World Context:
9. Where else is facial recognition used, and what are the implications?
   * Border/Immigration Control: Faster processing but privacy concerns
   * Retail Stores: Tracking shoplifters but constant surveillance
   * Airports: TSA PreCheck speedup but mass surveillance
   * Dating Apps: Verifying user identity but enabling stalking
   * Schools: Attendance tracking but privacy concerns for minors
   * China: Mass surveillance using billions of cameras; controlling population movement
10. How should society balance facial recognition's efficiency benefits with its risks?
   * Consider: Should accuracy requirements vary by use case?
   * Consider: Should it require explicit consent?
   * Consider: How do we prevent mission creep (starting for serious crime, expanding to minor offenses)?
   * Consider: How do we ensure accountability when systems make mistakes?
________________


CASE STUDY 3: AI-Assisted Medical Diagnosis
"Can AI Doctor Your Doctor?"
Time Allocation: 12 minutes | Reading: 2-3 min | Explanation: 3-5 min | Discussion: 5-10 min
Scenario Summary (2-3 minutes reading)
The Situation: Dr. Sarah Chen works in a radiology department. Each day, she reviews hundreds of X-rays, CT scans, and MRIs looking for tumors, fractures, infections, or other abnormalities. It's mentally demanding work requiring 12+ years of specialized training. Miss one tumor, and a patient's treatment is delayed dangerously.
Six months ago, the hospital implemented an AI diagnostic assistant. Now, when Dr. Chen uploads a chest X-ray, the AI highlights suspicious regions and assigns confidence scores: "97% confidence: pneumonia; 73% confidence: nodule consistent with malignancy; 34% confidence: cardiac abnormality."
The Reality:
* The AI catches 89% of pneumonia cases; Dr. Chen catches 91%
* The AI catches 85% of early-stage lung cancers; Dr. Chen catches 87%
* When they disagree, Dr. Chen is right 63% of the time; the AI is right 37%
* But when they agree on something normal, they're almost never wrong
The Impact:
* Hospital eliminated one radiologist position to save money (AI replaced a person)
* Remaining radiologists work 12-hour shifts reviewing the AI's flags
* Some radiologists rubber-stamp AI recommendations without careful review
* Some doctors resist using AI, fearing they'll lose expertise or autonomy
* Diagnostic errors actually increased slightly (from 0.8% to 1.2%)
The Question: The AI was supposed to improve patient care. Did it? For whom? And at what cost?
________________


AI Concept Explanation (3-5 minutes)
This Is Limited Memory AI + Supervised Deep Learning
Medical diagnosis with AI represents Limited Memory AI (IA de Memoria Limitada) powered by supervised deep learning.
Why Limited Memory?
The system reviews individual images (X-ray, CT scan) and provides recommendations for that image. It doesn't:
* Maintain ongoing patient history beyond what's in the current image
* Remember previous interactions with this patient
* Learn or adapt from this specific case after diagnosis
* Understand the patient's broader health context
* Apply reasoning across different medical domains
It's "limited" because it operates on current inputs without accumulated learning or reasoning.
Why Supervised Deep Learning?
Training Process:
Step 1: Data Collection
        Collect 100,000+ medical images (X-rays, CTs, MRIs)
        Each image is labeled by expert radiologists:
        "This image shows: pneumonia"
        "This image shows: normal"
        "This image shows: stage 3 lung cancer"


Step 2: Neural Network Architecture
        Input: Raw pixel values from medical image
        
        Layer 1: Detects low-level features
                (edges, boundaries, densities)
        
        Layer 2-5: Builds hierarchical features
                  (small regions looking like vessels, nodules)
        
        Layer 6-8: Recognizes disease patterns
                  (pneumonia consolidation pattern, cancer appearance)
        
        Output: Classification probabilities
               "89% pneumonia; 11% normal"


Step 3: Training (Backpropagation)
        Compare AI's prediction to expert label
        Adjust neural network weights to improve accuracy
        Repeat with thousands of images


Step 4: Testing
        Evaluate on unseen test images
        Report accuracy, sensitivity, specificity


Key Characteristic: Supervised Learning
* System has "correct answers" (labels from expert radiologists)
* Learns mapping from images to diagnoses
* Cannot learn from unlabeled images
* Requires continuous human expertise for training
The Narrow AI Challenge in Medicine
Medical AI is Narrow AI in several ways:
1. Task Specific: Trained to diagnose pneumonia; requires retraining for tuberculosis
2. Domain Specific: Chest X-ray AI cannot analyze MRI brain scans
3. Population Specific: Trained on certain demographics; may perform worse on others
4. Context Limited: Doesn't understand "this 35-year-old has kidney disease" (might affect diagnosis reasoning)
Why This Matters: Doctors have General Intelligence—they integrate across domains, apply broad medical knowledge, consider patient context, adjust reasoning for edge cases.
AI has Narrow Expertise—it excels at pattern recognition on its specific trained task but lacks flexibility.
The Human-AI Collaboration Challenge:
The ideal scenario combines:
* AI Strengths: Never tired, perfect recall of training data, detects subtle patterns
* Human Strengths: Context awareness, ethical reasoning, communication, handling novel situations
The Problem: When radiologists work with AI, they often:
* Rubber-stamp AI recommendations (over-trust)
* Completely ignore AI recommendations (under-trust)
* Lose practice and intuition if relying too heavily on AI
* Experience alert fatigue (too many flags to review carefully)
________________


Discussion and Reflection Questions (5-10 minutes)
Understanding Questions:
1. Why is medical diagnostic AI considered "Limited Memory" rather than "Reactive" AI?
   * Reactive AI: Responds only to current input; no memory at all
   * Limited Memory AI: Uses recent data/context to inform current decisions
   * In medical context: The AI considers patient's current symptoms, imaging, labs; reviews recent scan comparison if available
   * Difference: A truly reactive system couldn't reference prior scans; limited memory can
2. Medical AI is trained on thousands of labeled examples. What type of learning is this?
   * Answer: Supervised Learning
   * Why? Each training image has an expert-provided label (diagnosis)
   * Could it be Unsupervised? Not really—we're not asking it to discover disease categories; we're teaching it to recognize known categories
3. Explain why medical AI is "Narrow" rather than "General" AI.
   * Narrow: Can diagnose pneumonia from chest X-rays but cannot diagnose other conditions or analyze different image types without retraining
   * General would be: A system that learns from pneumonia diagnosis and generalizes that learning to diagnose other chest diseases, then even brain diseases, without retraining
   * Reality: Current medical AI requires separate training for each task
Analysis Questions:
4. The case study shows the AI catches 89% of pneumonia but Dr. Chen catches 91%. Does this mean the AI is worse?
   * Not necessarily. Consider:
      * Different types of errors: AI misses some cases but might also have false positives
      * Complementarity: Together they might catch 95%+ if they use each other's judgments well
      * Fatigue factor: After 8 hours, Dr. Chen might drop to 85% accuracy; AI stays at 89%
      * Cost: AI costs much less than hiring additional radiologists
   * Key insight: AI and humans have different error patterns; combination can outperform either alone
5. When the hospital eliminated a radiologist position, was that a good decision?
   * Arguments for: Cost savings; AI handles routine cases; radiologists do more complex cases
   * Arguments against:
      * Diagnostic accuracy actually worsened (error rate increased)
      * Remaining radiologists overworked and likely rushing
      * Loss of expertise in workforce
      * No guarantee AI will continue to work as well with overworked supervisors
   * Better approach: Use AI to enhance radiologists' productivity, not replace them; keep full staff; reduce burnout; improve accuracy
6. Some radiologists rubber-stamp AI recommendations without careful review. Why is this problematic?
   * This contradicts the goal. If AI is supposed to augment human judgment, but humans just accept it, then it's functioning as primary decision-maker
   * Risk: AI systematically makes same error (e.g., high false-positive rate), radiologist doesn't catch it
   * Why does it happen? Time pressure, alert fatigue, misplaced trust in technology
   * Solution: Training, workflow design, explicit policies requiring careful review
Ethical & Practical Discussion:
7. Who bears responsibility if AI-assisted diagnosis misses a cancer and the patient's treatment is delayed?
   * Perspectives:
      * Patient: Harmed; entitled to compensation
      * Radiologist: Should have caught it (AI is tool, not replacement)
      * Hospital/AI Company: Responsibility for ensuring safe deployment
      * AI Designer: Responsibility for transparent accuracy reporting
   * Legal reality: Varies by jurisdiction; often falls on radiologist and hospital
   * Ethical reality: All parties share responsibility
8. Should patients be told their diagnosis was made with AI assistance?
   * Arguments for disclosure:
      * Transparency and autonomy (patients can choose)
      * Informed consent (might affect trust in treatment)
      * Legal/ethical obligation for transparency
   * Arguments against disclosure:
      * Might undermine confidence in treatment
      * AI is tool; might be no different than "computed by software"
      * Might trigger legal liability concerns
   * Emerging best practice: Disclose when AI plays significant role; be honest about its capabilities and limitations
9. How would you design an AI system to truly "augment" radiologists rather than replace them?
   * Consider:
      * Explainability: Show radiologist why AI flagged something ("Region X has 73% pixel density matching nodule patterns")
      * Confidence indicators: "High confidence finding" vs. "low confidence; requires human review"
      * Error rate reporting: "On similar images, this AI misses 11% of nodules"
      * Overridability: Easy for radiologist to reject or modify AI recommendation
      * Feedback loop: System learns from radiologist overrides
      * Workload management: AI handles routine normal cases, flags complex cases for radiologist expertise
Real-World Applications:
10. Where else is diagnostic AI being used, and what are the implications?
   * Pathology: Analyzing tissue samples for cancer
   * Cardiology: Detecting heart disease from EKG or imaging
   * Dermatology: Identifying skin cancers from photos
   * Ophthalmology: Detecting diabetic retinopathy
   * Mental Health: Crisis detection from text/voice patterns
11. Common implications: Improves access (rural areas), enhances accuracy (fresh eyes), raises questions about job displacement, requires careful validation especially across different populations
________________


COMPARING THE THREE CASE STUDIES
Aspect
	Netflix
	Facial Recognition
	Medical Diagnosis
	AI Type
	Machine Learning (Collaborative Filtering)
	Narrow AI + Deep Learning
	Narrow AI + Supervised Deep Learning
	Learning Type
	Supervised + Reinforcement
	Supervised
	Supervised
	Memory
	Limited Memory (recent history)
	Reactive (current image only)
	Limited Memory (current scan + recent history)
	Primary Challenge
	Filter bubbles; privacy
	Bias; false arrests; privacy
	Accuracy variation; integration with workflow
	Ethical Concern
	Manipulation; surveillance
	Discrimination; false positives
	Responsibility; transparency; job displacement
	Human Role
	User-controlled choice
	Critical verification needed
	Collaboration & oversight essential
	________________


KEY INSIGHTS FROM CASE STUDIES
1. AI Success Requires Proper Context:
   * Netflix recommendations work because convenience > privacy concerns for many users
   * Facial recognition needs human verification due to bias and accuracy limitations
   * Medical AI needs radiologist oversight—can't be fully automated
2. Narrow AI Limitations Are Real:
   * Each system excels in its narrow domain
   * All fail or perform poorly outside training context
   * This is current reality; we're far from General AI
3. "Better Than Humans" Doesn't Mean "Good Enough":
   * Netflix AI might recommend 87% as accurately as a human; that's fine for entertainment
   * Facial recognition at 94% accuracy sounds good but produces false arrests; not acceptable
   * Medical AI at 89% accuracy requires careful integration with human radiologists
4. Ethics Requires Transparency and Accountability:
   * Users should understand systems making decisions about them
   * Consequences determine how rigorous testing needs to be
   * Someone must be responsible when systems fail
5. Technology Doesn't Solve Underlying Problems:
   * Facial recognition bias reflects historical policing bias (data problem, not just algorithm)
   * Medical AI inequality reflects healthcare system inequality (implementation problem)
   * Netflix filter bubbles reflect human tendency to like similar content (psychology problem)
________________


END OF CASE STUDIES
Total Time: 30 minutes (12 min × 3 case studies)
Format: Each case study progresses from concrete scenario → conceptual explanation → critical reflection
Bilingual Terms Used:
* Inteligencia Artificial (IA)
* Aprendizaje Automático (ML)
* Aprendizaje Profundo (DL)
* Red Neuronal Convolucional (CNN)
* IA Estrecha (Narrow AI)
* IA de Memoria Limitada (Limited Memory AI)
* Aprendizaje Supervisado (Supervised Learning)
Assessment Readiness: These case studies prepare learners for quiz questions about real-world AI applications and critical thinking about AI limitations and ethics.