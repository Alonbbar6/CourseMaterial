Module 10: Capstone Project: Mini AI Application
Peer Review/Feedback Framework (10% - Structured Peer Evaluation)
________________


Overview
The peer review component is a critical part of the capstone experience. It provides learners with:
* Constructive feedback from peers on their project
* Exposure to different solutions to similar problems
* Professional communication skills in giving/receiving feedback
* Quality assurance ensuring projects meet standards
* Community learning - learning from each other's approaches
Time Allocation: 10 minutes per learner (peer reviews)
* 5 minutes: Review peer's project (code + documentation)
* 3 minutes: Complete feedback form
* 2 minutes: Provide verbal/written summary
Total Peer Review Process: ~15-20 minutes per learner across multiple feedback cycles
________________


Peer Review Structure
Phase 1: Preparation (Before Peer Review)
What students do before receiving feedback
Phase 2: Peer Review Assignment
Matching students with reviewers
Phase 3: Review Execution
Conducting the actual review
Phase 4: Feedback Delivery
Providing constructive feedback
Phase 5: Response & Iteration
Learner responds to feedback
________________


PHASE 1: PREPARATION (Learner Submits Project)
1A: Project Submission Checklist
Before a learner's project is ready for peer review, it must pass this submission checklist:
Code Quality Checklist:
☐ Code runs without errors
☐ All functions have docstrings
☐ Variable names are clear and descriptive
☐ Comments explain complex logic
☐ Error handling is implemented
☐ Code follows consistent formatting
☐ No hardcoded credentials/secrets
☐ Dependencies listed in requirements.txt


Documentation Checklist:
☐ README.md is complete and clear
☐ Problem statement is included
☐ Setup instructions are detailed
☐ Usage example is provided
☐ Results/outcomes are documented
☐ Challenges and learnings section included
☐ Future improvements listed
☐ File structure is organized


Testing Checklist:
☐ Code has been tested with multiple inputs
☐ Edge cases have been considered
☐ Error scenarios handled gracefully
☐ Test results documented
☐ All tests pass successfully


Files Submitted:
☐ main.py (or equivalent primary file)
☐ config.py or settings file
☐ requirements.txt
☐ README.md
☐ Test file (test_*.py)
☐ Sample data/images/prompts used
☐ Output examples (results.json, generated_content.txt, etc.)
☐ .env template (without actual credentials)


Submission Portal Format
Students submit a ZIP file containing:
StudentName_Capstone_Project/
├── README.md (READ THIS FIRST)
├── main.py
├── config.py
├── requirements.txt
├── test_project.py
├── data/
│   └── sample_data
├── output/
│   └── sample_results
└── PROJECT_SUMMARY.txt (1-page project overview)


PROJECT_SUMMARY.txt template:
PROJECT TITLE: [Your Project Name]
STUDENT NAME: [Your Name]
PROJECT OPTION: [1: Text Classifier / 2: Image Analyzer / 3: Email Generator]


PROBLEM SOLVED:
[1-2 sentences about the problem your project solves]


KEY FEATURES:
- Feature 1
- Feature 2
- Feature 3


TECHNOLOGIES USED:
- Python 3.x
- [API/Library names]


RESULTS:
[Brief description of what your project achieves]


TIME SPENT:
- Setup: X minutes
- Implementation: X minutes
- Testing: X minutes
- Documentation: X minutes
- Total: 84 minutes


KNOWN LIMITATIONS:
[Any limitations or edge cases your project doesn't handle]


________________


PHASE 2: PEER REVIEW ASSIGNMENT
2A: Reviewer Pairing Strategy
Strategy 1: Same-Track Pairing (Recommended)
* Text Classifier reviewers review other Text Classifiers
* Image Analyzer reviewers review other Image Analyzers
* Email Generator reviewers review other Email Generators
* Advantage: Reviewers understand the domain deeply
* Advantage: Fair comparison of implementations
Strategy 2: Cross-Track Pairing
* Each project reviewed by someone who built a different project
* Advantage: Exposes learners to different solutions
* Advantage: Diverse feedback perspectives
* Disadvantage: Reviewers may need more time to understand domain
Strategy 3: Random Pairing
* Projects randomly assigned to reviewers
* Advantage: Completely unbiased
* Disadvantage: May result in mismatches
Recommended Process:
1. Use Same-Track Pairing (Strategy 1) for primary review
2. Optionally use Cross-Track Pairing for secondary review
3. Each project receives 2-3 reviews
Example Assignment Matrix
TEXT CLASSIFIER REVIEWERS:
├─ StudentA (Text Classifier) → reviews StudentB's Text Classifier
├─ StudentB (Text Classifier) → reviews StudentC's Text Classifier
├─ StudentC (Text Classifier) → reviews StudentA's Text Classifier
└─ StudentD (Text Classifier) → reviews StudentE's Text Classifier


IMAGE ANALYZER REVIEWERS:
├─ StudentF (Image Analyzer) → reviews StudentG's Image Analyzer
├─ StudentG (Image Analyzer) → reviews StudentH's Image Analyzer
└─ StudentH (Image Analyzer) → reviews StudentF's Image Analyzer


EMAIL GENERATOR REVIEWERS:
├─ StudentI (Email Generator) → reviews StudentJ's Email Generator
└─ StudentJ (Email Generator) → reviews StudentI's Email Generator


________________


PHASE 3: REVIEW EXECUTION
3A: Peer Reviewer Role
What is a Peer Reviewer?
A peer reviewer is a fellow learner who:
* Examines another student's capstone project
* Tests if the code runs
* Reads and evaluates the documentation
* Provides constructive feedback
* Suggests improvements
* Identifies strengths and potential issues
Code Review Process (Step-by-Step)
Step 1: Environment Setup (2 minutes)
☐ Extract the ZIP file
☐ Read PROJECT_SUMMARY.txt first
☐ Scan the README.md for overview
☐ Look at file structure
☐ Read through main code file


Step 2: Setup & Execution (2 minutes)
☐ Follow setup instructions from README
☐ Install dependencies: pip install -r requirements.txt
☐ Set up API credentials if needed
☐ Run the main program: python main.py
☐ Verify program runs without errors
☐ Check that output is generated


Step 3: Code Quality Review (2 minutes)
☐ Is code readable and well-formatted?
☐ Are variable names clear?
☐ Do functions have docstrings?
☐ Are comments helpful?
☐ Is error handling present?
☐ Are there any obvious bugs?


Step 4: Feature Testing (2 minutes)
☐ Try different inputs
☐ Test edge cases
☐ Check if program handles errors gracefully
☐ Verify all advertised features work
☐ Look for any crashes or unexpected behavior


Step 5: Documentation Review (1 minute)
☐ Is README clear and complete?
☐ Are setup instructions accurate?
☐ Is usage example helpful?
☐ Are results clearly documented?
☐ Are there future improvements listed?


Total Review Time: ~9 minutes
________________


3B: Peer Review Checklist Template
CAPSTONE PROJECT PEER REVIEW FORM
Reviewer Name: ________________
Project Reviewed: ________________
Date: ________________
Time Spent: ________ minutes
________________


SECTION 1: CAN YOU RUN IT?
Q1.1: Did the project run without errors?
☐ Yes - ran perfectly
☐ Mostly - minor warnings
☐ No - encountered errors


Q1.2: If errors occurred, describe them:
[Describe any errors encountered]


Q1.3: How long did setup take?
☐ < 2 minutes (excellent)
☐ 2-5 minutes (good)
☐ 5-10 minutes (acceptable)
☐ > 10 minutes (needs improvement)


________________


SECTION 2: CODE QUALITY
Q2.1: Code Organization
Rating: ☐ Excellent ☐ Good ☐ Fair ☐ Needs Work


What works well:
[Specific examples of good organization]


What could improve:
[Specific suggestions for organization]


Q2.2: Readability & Naming
Rating: ☐ Excellent ☐ Good ☐ Fair ☐ Needs Work


Specific examples:
- Well-named variables: [Examples]
- Variables needing better names: [Examples]


Q2.3: Documentation in Code
Rating: ☐ Excellent ☐ Good ☐ Fair ☐ Needs Work


Examples of good documentation:
[Quote specific functions with good docstrings]


Areas missing documentation:
[Functions or sections lacking comments]


Q2.4: Error Handling
Rating: ☐ Excellent ☐ Good ☐ Fair ☐ Needs Work


Examples of good error handling:
[Specific try-catch blocks or validations]


Potential issues if errors occur:
[Scenarios where error handling is missing]


________________


SECTION 3: FUNCTIONALITY
Q3.1: Core Features Working
Feature 1: [Feature name]
Status: ☐ Works perfectly ☐ Works mostly ☐ Doesn't work


Feature 2: [Feature name]
Status: ☐ Works perfectly ☐ Works mostly ☐ Doesn't work


Feature 3: [Feature name]
Status: ☐ Works perfectly ☐ Works mostly ☐ Doesn't work


Q3.2: Edge Case Handling
Test Case: Empty input
Result: ☐ Handled well ☐ Handled okay ☐ Crashes


Test Case: Unusual input
Result: ☐ Handled well ☐ Handled okay ☐ Crashes


Test Case: Large input volume
Result: ☐ Handles well ☐ Handles okay ☐ Times out


Q3.3: Performance
Estimated runtime for typical use:
☐ < 1 second (excellent)
☐ 1-5 seconds (good)
☐ 5-30 seconds (acceptable)
☐ > 30 seconds (slow)


Any performance issues:
[Describe bottlenecks or slow operations]


________________


SECTION 4: DOCUMENTATION QUALITY
Q4.1: README Completeness
☐ Problem statement clear
☐ Solution well explained
☐ Setup instructions complete
☐ Usage example provided
☐ Results documented
☐ Challenges/learnings included
☐ Future improvements listed


Missing sections:
[Which sections are incomplete or missing?]


Q4.2: README Clarity
Overall clarity: ☐ Excellent ☐ Good ☐ Fair ☐ Confusing


Hardest part to understand:
[What was unclear?]


Best part of the documentation:
[What explains things well?]


Q4.3: Setup Instructions Accuracy
☐ I was able to follow instructions
☐ Instructions had minor issues
☐ Instructions were confusing
☐ Instructions didn't work


Specific issues (if any):
[Describe any problems with setup]


________________


SECTION 5: PROJECT REQUIREMENTS
Q5.1: Problem-Solution Fit
Does the solution actually solve the stated problem?
☐ Yes, completely
☐ Mostly
☐ Somewhat
☐ No


How well does it address the problem? (1-5 stars)
★★★★★ ☐  ★★★★☐ ☐  ★★★☐☐ ☐  ★★☐☐☐ ☐  ★☐☐☐☐ ☐


Q5.2: Success Criteria Met
Was the project successful? (Based on stated criteria)
☐ Exceeded all criteria
☐ Met all criteria
☐ Met most criteria
☐ Met some criteria
☐ Didn't meet criteria


Which criteria were best met:
[Examples]


Which criteria fell short:
[Examples]


Q5.3: Scope Appropriateness
Was the scope appropriate for 84 minutes?
☐ Well-scoped - used time effectively
☐ Good scope - reasonable features
☐ Over-scoped - too ambitious
☐ Under-scoped - too simple


Suggestions for scope adjustment:
[What could be added/removed?]


________________


SECTION 6: TECHNICAL DECISIONS
Q6.1: Technology Choices
Are the technologies used appropriate?
☐ Excellent choices - well justified
☐ Good choices - make sense
☐ Acceptable - other options would work too
☐ Questionable - better tools available


Explanation:
[Why are the choices good/problematic?]


Q6.2: Code Architecture
Is the code well-structured?
☐ Excellent - professional structure
☐ Good - clear organization
☐ Fair - some organizational issues
☐ Needs work - confusing structure


Suggestions for improvement:
[How could code be better organized?]


________________


SECTION 7: STRENGTHS
Q7.1: What This Project Does Best (List 3-5 strengths)
✓ Strength 1: [What the project does particularly well]
* Example: [Specific evidence]
✓ Strength 2: [What the project does particularly well]
* Example: [Specific evidence]
✓ Strength 3: [What the project does particularly well]
* Example: [Specific evidence]
________________


SECTION 8: AREAS FOR IMPROVEMENT
Q8.1: Specific Suggestions (List 3-5 improvements)
→ Suggestion 1: [What could be improved]
* How: [Specific approach to fix]
* Why: [Why this would help]
→ Suggestion 2: [What could be improved]
* How: [Specific approach to fix]
* Why: [Why this would help]
→ Suggestion 3: [What could be improved]
* How: [Specific approach to fix]
* Why: [Why this would help]
________________


SECTION 9: LEARNING & CREATIVITY
Q9.1: Evidence of Learning
Does the project show understanding of course concepts?
☐ Excellent - demonstrates mastery
☐ Good - clear understanding
☐ Fair - some gaps in understanding
☐ Needs work - fundamental gaps


Examples of learning demonstrated:
[Specific areas where student shows competency]


Q9.2: Creativity & Original Thinking
Does the project show original thinking?
☐ Highly creative approach
☐ Some creative elements
☐ Straightforward implementation
☐ Basic/generic approach


Creative elements:
[What makes this project unique?]


Q9.3: Problem-Solving Approach
How well does the student solve problems?
☐ Excellent - elegant, efficient solutions
☐ Good - effective solutions
☐ Fair - works but could be more elegant
☐ Basic - gets the job done


Example of good problem-solving:
[Describe a specific solution that impresses]


________________


SECTION 10: OVERALL ASSESSMENT
Q10.1: Overall Project Quality
Rate the overall quality: ☐ Excellent ☐ Good ☐ Fair ☐ Needs Work


Justification:
[Why did you rate it this way?]


Q10.2: Would This Be Useful in Real-World?
☐ Yes - I would actually use this
☐ Mostly - With some refinement
☐ Somewhat - Concept is good
☐ No - Too limited/broken


Why or why not:
[Explain your reasoning]


Q10.3: Recommendation
This project:
☐ Exemplary - Share with class/portfolio
☐ Strong - Meets expectations
☐ Satisfactory - Meets requirements
☐ Needs Revision - Requires rework


Suggested next steps for learner:
[What should they do next?]


________________


SECTION 11: PEER FEEDBACK SUMMARY
Summary for Learner (2-3 paragraphs):
What I liked most about your project:
[Summarize the 2-3 main strengths you identified]
[Be specific and genuine in your praise]
[Reference specific code examples or features]


What would make your project even better:
[Summarize the 2-3 main improvement suggestions]
[Prioritize by impact - what matters most?]
[Provide actionable, specific suggestions]


One key learning insight:
[What did you learn by reviewing this project?]
[What will you apply to your own work?]
[Any techniques you'd like to use in the future?]


________________


SECTION 12: REVIEWER SIGNATURE
Reviewer Name: ________________
Reviewer Email: ________________
Date Completed: ________________
Time Spent Reviewing: ________ minutes
Reviewer Statement: I have reviewed this project thoroughly and provided honest, constructive feedback intended to help the learner improve their work.
Signature: ________________
________________


PHASE 4: FEEDBACK DELIVERY
4A: Giving Good Feedback (Framework)
The Feedback Sandwich (But Better)
Traditional Sandwich (Outdated):
Positive → Criticism → Positive (Feels fake)


Better Approach - Specific & Actionable:
Format: Observation → Impact → Suggestion
Example 1:
❌ BAD: "Your code is messy."


✓ GOOD: "I noticed the calculate_sentiment() function has 15 
nested conditions without intermediate variables. This makes it 
hard to debug. Consider extracting conditions into named boolean 
variables like 'has_positive_keywords' to improve readability."


Example 2:
❌ BAD: "Good job on the README."


✓ GOOD: "Your setup instructions were easy to follow. The 
step-by-step format with code blocks made it simple to get 
everything installed. This helped me run your project successfully 
on the first try."


The 5 Feedback Rules
Rule 1: Be Specific
❌ Vague: "The code is good"
✓ Specific: "The error handling in process_review() is excellent 
           - you check for empty input, invalid files, and API 
           timeouts. This prevented crashes on edge cases."


Rule 2: Reference Examples
❌ Generic: "Comments are helpful"
✓ With Example: "Your comment before the caching logic 'Store 
               results to avoid redundant API calls' explains 
               the 'why' not just the 'what', which is helpful."


Rule 3: Balance Positives & Areas to Improve
Format: 60% Positive, 40% Constructive
- 2-3 Strengths
- 2-3 Improvements
- 1 "Keep doing this" feedback


Rule 4: Be Actionable
❌ Non-actionable: "Performance could be better"
✓ Actionable: "The image analysis takes 5 seconds per image. 
            You could cache results (if image hasn't changed) 
            or parallelize processing. Start with caching since 
            it requires fewer changes."


Rule 5: Assume Good Intent
❌ Accusatory: "You didn't test edge cases"
✓ Collaborative: "Edge case testing is hard to think of upfront. 
               One suggestion: test with empty strings, very long 
               input, and special characters. Your error handling 
               would catch these."


________________


4B: Feedback Categories
Category 1: Strengths (Be Specific & Genuine)
Template:
Your project excels at [specific thing]. I noticed [specific evidence].
This is valuable because [why it matters].


Example application: [How they could use this skill elsewhere]


Real Examples:
✓ Your project excels at error handling. I noticed every function 
  checks for invalid input and returns meaningful error messages. 
  This is valuable because it prevents crashes and helps users 
  understand what went wrong.


✓ Your documentation is outstanding. The setup instructions include 
  troubleshooting for common issues. This saved me 10 minutes of 
  debugging. I borrowed this approach for my own README.


✓ The code is elegantly written. You used list comprehensions and 
  generator functions efficiently, making the code both readable 
  and performant.


Category 2: Technical Improvements
Template:
In [location], you could [suggestion]. This would [benefit].
Here's one approach: [specific how-to]


Real Examples:
→ In the generate_email() function, you could add timeout handling 
  for API calls. This would prevent the program from hanging if the 
  API is slow. Here's one approach: wrap the request in a try-except 
  catching timeout exceptions, then implement exponential backoff retry.


→ The image_paths list is recreated every time process_directory() 
  is called. You could optimize by caching this list. This would be 
  faster for repeated runs. Here's one approach: store the list as 
  an instance variable and only regenerate if directory changes.


Category 3: Documentation Improvements
Template:
The README could clarify [topic]. Specifically, [what's unclear].
Consider adding [suggestion].


Real Examples:
→ The README could clarify the difference between "confidence" and 
  "magnitude" in sentiment scores. Specifically, many users won't 
  know what magnitude means. Consider adding a brief explanation 
  with examples: "Magnitude measures emotion intensity regardless 
  of polarity. A strongly negative and strongly positive review 
  both have high magnitude."


→ The setup instructions could mention what to do if API calls fail. 
  Consider adding a troubleshooting section with common errors like 
  "ImportError: No module named google.cloud" and their solutions.


Category 4: Feature Suggestions
Template:
You could enhance [feature] by [suggestion]. This would [benefit].
For implementation: [brief technical approach]


Real Examples:
→ You could enhance the text classifier by adding batch results 
  comparison. This would help users see trends (e.g., 70% positive 
  reviews this month vs. 60% last month). For implementation: 
  store multiple result files with timestamps, then write a 
  comparison function.


→ You could enhance the email generator by adding tone templates. 
  This would let users choose "urgent," "warm," "apologetic" tones 
  beyond the current options. For implementation: create a TONE_TEMPLATES 
  dictionary with prompt modifiers for each tone.


Category 5: Learning Observations
Template:
Your project shows strong understanding of [concept]. Specifically, 
[evidence]. This demonstrates [skill].


Real Examples:
✓ Your project shows strong understanding of API integration. 
  Specifically, you handled authentication, rate limiting, and 
  error responses professionally. This demonstrates you could 
  integrate any third-party API.


✓ Your project shows creative problem-solving. The workaround for 
  image size limitations using PIL resizing was clever. This 
  demonstrates adaptability when facing technical constraints.


________________


PHASE 5: RESPONSE & ITERATION
5A: Learner Response Process
How Learners Should Respond to Feedback
Step 1: Read the Feedback (Don't React Immediately)
☐ Read all feedback
☐ Don't respond emotionally
☐ Sleep on it if you feel defensive
☐ Separate "nice to hear" from "actionable"


Step 2: Categorize the Feedback
High Priority (Fix First):
- [Feedback items that fix bugs or critical issues]


Medium Priority (Nice to Have):
- [Improvements that add value]


Low Priority (Future Versions):
- [Features or nice-to-haves for v2.0]


Step 3: Create an Action Plan
For Each High-Priority Item:
1. What feedback am I acting on?
2. Why is this important?
3. What changes will I make?
4. How will I test this change?
5. Time estimate to complete


Example Action Plan:
Feedback: "Consider adding timeout handling for API calls"


1. Feedback: API calls could hang indefinitely
2. Why: Users won't know if program is stuck or waiting
3. Changes: Wrap API calls in try-except for timeout, add 30s limit
4. Test: Simulate slow API with time.sleep(), verify program recovers
5. Time: 15 minutes


Step 4: Implement Changes
☐ Make code changes
☐ Test thoroughly
☐ Update documentation
☐ Commit changes to version control


Step 5: Respond to Reviewers
Create FEEDBACK_RESPONSE.md:


For each reviewer:
1. Thank them specifically
2. Summarize feedback you're acting on
3. Explain why you're acting on it
4. Describe changes you'll make
5. Ask questions if you need clarification


________________


Example Feedback Response
# Feedback Response - [Your Name]


## Reviewer: Jane Doe
**Date**: January 15, 2024


### Thank You
Thank you for the thorough review of my text classifier project. 
Your specific observations and actionable suggestions were incredibly 
helpful.


### Feedback I'm Implementing


**1. "Error handling could be more robust"**
- **Your suggestion**: Handle API timeouts and rate limiting
- **Why I'm doing this**: Without timeout handling, users won't know 
  if the program is hung or slow. This is a critical user experience issue.
- **What I'll do**: Wrap API calls in try-except blocks, implement 
  30-second timeout, add exponential backoff for rate limiting
- **Timeline**: Complete by Jan 20
- **Testing**: Simulate slow API responses and verify recovery


**2. "Performance could be optimized with caching"**
- **Your suggestion**: Cache repeated review classifications
- **Why I'm doing this**: Review texts often repeat in large datasets. 
  Caching prevents redundant API calls, saving money and time.
- **What I'll do**: Add dictionary cache, use text hash as key, check 
  cache before API calls
- **Timeline**: Complete by Jan 22
- **Testing**: Run 100 reviews (20 duplicates), measure time/cost


### Feedback I'm Considering for Future Versions


**1. "Multi-language support would be valuable"**
- **Your suggestion**: Extend to Spanish, French, etc.
- **Why I'm deferring**: This is scope creep for this project, but 
  would be perfect for v2.0. Google Cloud NL already supports 10+ 
  languages, so this is feasible.
- **Future timeline**: Phase 2 (in 1 month)


### Questions for You


1. For exponential backoff: should I use 2^n or 1.5^n? Any preference?
2. Should cached results include timestamp for cache invalidation?


### Timeline for Updates
- v1.1 (Error handling + caching): Jan 20-22
- Updated project submitted: Jan 25
- I'll send you the updated code for feedback on improvements


Thank you again for the detailed review!


________________


5B: Instructor/TA Role in Peer Review
Monitoring Peer Reviews
What Instructors Should Check:
☐ All students are providing reviews (not skipping)
☐ Reviews are substantive (not just "looks good!")
☐ Reviews are respectful and constructive
☐ Reviewers are providing specific feedback
☐ Review time is reasonable (~5-10 minutes)
☐ No significant knowledge gaps in reviews


Red Flags:
🚩 Generic feedback ("Nice job!", "Good work!")
🚩 Overly critical or harsh tone
🚩 Reviews that are too brief (< 2 minutes of work)
🚩 Reviewers who don't actually run the code
🚩 Feedback that's not actionable
🚩 Clearly didn't read the code/documentation


Instructor Interventions
If Review Quality Is Low:
1. Provide feedback to reviewer on how to give better feedback
2. Share example of "good peer review"
3. Suggest specific things to look for
4. Consider re-assigning reviewer or having them redo review


If There's Conflict:
1. Read both sides carefully
2. Determine if feedback is fair or unfair
3. If unfair: explain to reviewer why
4. If fair but harsh: suggest softer wording
5. Help learner understand the feedback is meant to help


________________


PEER REVIEW SCHEDULE & TIMELINE
Example Implementation: 5-Person Class
Total Class Size: 5 learners
Projects: 2 Text Classifiers, 2 Image Analyzers, 1 Email Generator
Review Model: Same-track pairing with 1 cross-track review
Week 1: Implementation
Monday-Wednesday: Learners build projects
Thursday: Projects submitted


Student A (Text Classifier) ──→ Submitted
Student B (Text Classifier) ──→ Submitted
Student C (Image Analyzer) ──→ Submitted
Student D (Image Analyzer) ──→ Submitted
Student E (Email Generator) ──→ Submitted


Week 2: Peer Review (First Round)
Friday (Session 1 - 45 minutes):


TIME    ACTIVITY
0:00    Introduction to peer review (5 min)
0:05    Reviewers assigned projects (2 min)
0:07    Reviewers download and extract (1 min)
0:08    Quiet review time (15 min)
        - Set up environment
        - Run code
        - Read documentation
        - Test features
        - Take notes
0:23    Review form completion (10 min)
0:33    Q&A with reviewers (5 min)
0:38    Collect feedback forms
0:45    End


Assignments:
- Student A reviews Student B (Text Classifier)
- Student B reviews Student A (Text Classifier)
- Student C reviews Student D (Image Analyzer)
- Student D reviews Student C (Image Analyzer)
- Student E reviews Student A (Cross-track: Email → Text)


Week 2: Feedback Delivery
Saturday (Email Distribution):
- Feedback forms sent to each learner
- Learners have until Monday to read and plan response
- No immediate action required


Week 3: Response & Iteration (Optional)
Monday-Wednesday:
- Learners implement high-priority improvements
- Update documentation if needed
- Test changes
- Create FEEDBACK_RESPONSE.md


Thursday (Optional Second Review):
- Submit revised projects
- Quick review by original reviewers
- "Looks good!" confirmation


Friday (Final Presentations):
- Learners present project + what they learned from feedback


________________


PEER REVIEW RUBRIC
For Grading Quality of Peer Reviews
Scoring Scale: 1-4 (Below Expectations, Meets Expectations, Exceeds Expectations)
Rubric for Peer Reviewer Performance
Criteria
	1 - Below
	2 - Meets
	3 - Exceeds
	4 - Exemplary
	Feedback Specificity
	Generic ("good job")
	Some specific examples
	Multiple specific code examples
	Examples with line numbers/quotes
	Actionability
	Feedback isn't actionable
	Some actionable suggestions
	Mostly actionable with steps
	Clear steps with implementation approach
	Balance
	Only criticism or only praise
	Some balance attempted
	Good balance of strengths/improvements
	Excellent balance with nuanced observations
	Evidence of Testing
	Unclear if code was tested
	Mentions running code
	Describes specific test attempts
	Documents testing with results
	Tone
	Harsh or dismissive
	Professional but dry
	Encouraging and constructive
	Genuinely supportive and collaborative
	Completeness
	Minimal feedback
	Covers major areas
	Covers most areas thoroughly
	Comprehensive with depth
	Understanding
	Misunderstands project
	Basic understanding
	Good understanding demonstrated
	Deep understanding of approach/challenges
	________________


PEER REVIEW EXAMPLES
Example 1: Good Peer Review
PROJECT REVIEWED: Text Classifier for Restaurant Reviews
REVIEWER: Alex Chen
DATE: Jan 15, 2024


SECTION 1: CAN YOU RUN IT?
Q1.1: Did the project run without errors?
✓ Yes - ran perfectly


Q1.3: How long did setup take?
✓ < 2 minutes (excellent)


---


SECTION 2-4: [Detailed feedback in each section]


---


SECTION 10-11: OVERALL ASSESSMENT & SUMMARY


Q10.1: Overall Project Quality
✓ Excellent


Justification: This project demonstrates solid understanding of API 
integration and best practices. The code is well-organized, the 
documentation is clear, and all features work as intended.


---


SUMMARY FOR LEARNER:


**What I liked most about your project:**


Your error handling is exceptional. I noticed you check for empty 
strings, API timeouts, and malformed responses. You even return 
informative error messages. This shows you've thought about real-world 
scenarios. Most importantly, when I deliberately fed bad data, the 
program handled it gracefully instead of crashing.


The README is outstanding. Your setup instructions are step-by-step 
and include troubleshooting. The usage example shows different tone 
options. This would help anyone get started quickly.


Your sentiment analysis results are surprisingly accurate. When I tested 
sarcastic reviews ("Great service... if you like waiting 30 minutes!"), 
the classifier correctly identified the negative sentiment despite the 
word "Great." This suggests good prompt engineering or API parameter 
tuning.


**What would make your project even better:**


Consider adding batch processing optimization. Right now you process 
each review sequentially, which is fine for 10 reviews but would be slow 
for 1000. You could use threading or async calls to process multiple 
reviews in parallel. This would make the tool production-ready.


The confidence scores could be explained better. I see percentages but 
don't know if 65% is "confident enough" for business decisions. Add a 
section in the README explaining confidence thresholds and 
recommendations.


Performance logging would be valuable. Adding a simple timer to track 
how long each API call takes would help users understand performance 
characteristics. You could save this to a JSON log file.


**One key learning insight:**


I learned that good error handling makes code production-ready. Before 
this review, I didn't think much about edge cases. But watching your 
project gracefully handle malformed input convinced me this is critical. 
I'm going to add similar error handling to my own project.


---


RATING: Exemplary - This project demonstrates mastery and would be a 
good portfolio piece to show employers.


Example 2: Feedback That Needs Improvement
❌ BAD REVIEW (Too Generic):


PROJECT REVIEWED: Image Analyzer
REVIEWER: Sam
DATE: Jan 15


Great job on the project! The code looks good and the README is helpful.
The image detection works well. Nice work on the documentation too.
Overall, I really liked this project. Good luck with future projects!


PROBLEMS:
- No specific examples
- No evidence of testing
- No constructive suggestions
- Doesn't help learner improve
- Too brief (< 5 minutes work)


________________


COMMON PEER REVIEW MISTAKES & HOW TO AVOID THEM
Mistake 1: Being Too Critical
❌ BAD: "Your code is poorly organized. The functions are all mixed 
       together and nobody could understand it."


✓ GOOD: "The functions are well-written individually, but could be 
       organized better. Currently, you have sentiment analysis, 
       data loading, and result display all mixed together. 
       Consider separating into distinct modules. This would make 
       the code easier to test and reuse."


Mistake 2: Being Too Vague
❌ BAD: "The documentation could be better."


✓ GOOD: "The documentation is mostly complete. The README explains 
       the problem and solution well. One area that could be clearer: 
       the 'Results' section shows numbers but doesn't explain what 
       makes a result 'good.' For example, is 75% accuracy acceptable? 
       Consider adding context or benchmarks."


Mistake 3: Not Actually Testing the Code
❌ BAD: "Looks good based on the code!"


✓ GOOD: "I ran the code with your sample data. The program executed 
       successfully and generated the expected output in 3 seconds. 
       I also tested with edge cases: empty file, very large file, 
       special characters. The error handling worked well in all cases."


Mistake 4: Suggesting Implementation Details You're Not Sure About
❌ BAD: "You should use async/await to make this faster."


✓ GOOD: "Performance could potentially be improved by processing 
       multiple images in parallel instead of sequentially. This could 
       be done with Python's threading module or async functions. 
       I'm not an expert in this area, but it's worth exploring if 
       speed becomes an issue."


Mistake 5: Forgetting to Acknowledge Strengths
❌ BAD: [Only lists problems and suggestions]


✓ GOOD: 
"STRENGTHS:
- Your error handling is robust
- The README is clear and helpful
- The code is readable


AREAS FOR IMPROVEMENT:
- [Suggestions here]"


________________


SAMPLE PEER REVIEW FEEDBACK PHRASES
For Strengths
"I particularly appreciated..."
"One thing that impressed me was..."
"Your approach to [feature] is excellent because..."
"The way you handled [situation] shows..."
"Your code clearly demonstrates understanding of..."
"This is production-quality work in terms of..."


For Constructive Feedback
"One suggestion would be to..."
"You might consider..."
"A potential improvement would be..."
"If I were using this in production, I would want..."
"Have you thought about...?"
"Another approach might be..."
"For future versions, you could..."


For Clarification
"I'm curious about your choice to..."
"Can you explain why you implemented...?"
"I noticed [something] - was this intentional?"
"What was the reasoning behind...?"
"I'd like to understand your approach to..."


For Encouragement
"Great effort on..."
"I'm impressed by your ability to..."
"This shows real growth in..."
"Keep focusing on [skill]..."
"You should be proud of..."


________________


PEER REVIEW POLICIES
Academic Integrity in Peer Review
Reviewers Must Not:
❌ Copy code from reviewed project
❌ Share project details outside review
❌ Use code/ideas without permission
❌ Give their own code to learner
❌ Complete the project for them


Reviewers Can:
✓ Suggest approaches (don't code)
✓ Point out existing patterns they know about
✓ Recommend libraries/tools
✓ Explain concepts the learner should research
✓ Provide general coding best practices


Confidentiality
Review Content is CONFIDENTIAL
- Don't share feedback publicly
- Don't compare projects with others
- Don't discuss implementation details
- Share only general insights
- Always ask permission before sharing code


Respectful Communication
✓ Assume best intentions
✓ Focus on behavior, not person ("The code could be organized better" not "You didn't organize well")
✓ Use "I" statements ("I found this confusing" not "This is confusing")
✓ Offer solutions not just criticism
✓ Acknowledge effort and learning


________________


METRICS & ANALYTICS
Tracking Peer Review Quality
Metrics to Collect:
1. Review Completion Rate
   - Percentage of students who completed reviews
   - Target: 100%


2. Review Time
   - Average time spent on each review
   - Target: 5-10 minutes per review


3. Feedback Specificity
   - Percentage of reviews with specific examples
   - Target: 80%+


4. Actionability Score
   - Percentage of feedback that was actionable
   - Target: 80%+


5. Learner Implementation Rate
   - Percentage of feedback that learners acted on
   - Target: 70%+


6. Learner Satisfaction
   - "Was the feedback helpful?" survey (1-5 scale)
   - Target: 4.0+/5.0


7. Code Quality Improvements
   - Measure improvements pre/post feedback
   - Target: 20%+ quality increase


________________


CAPSTONE PROJECT EVALUATION SUMMARY
How Peer Reviews Impact Overall Grade
Capstone Project Grading Breakdown:
├─ Project Execution (60%)
│  ├─ Does it work? (40%)
│  ├─ Code quality (20%)
│  └─ Innovation/Creativity (included in above)
├─ Documentation (20%)
│  ├─ README quality (10%)
│  └─ Code comments (10%)
├─ Peer Review (10%)
│  ├─ Quality of reviews given (5%)
│  └─ How well learner addressed feedback (5%)
└─ Presentation (10%)
   ├─ Communication (5%)
   └─ Understanding demonstrated (5%)


TOTAL: 100%


________________


This completes the 10% Peer Review/Feedback framework for Module 10!
Key Components: ✅ Submission checklist ✅ Reviewer assignment strategies ✅ Step-by-step review process ✅ Comprehensive feedback form template ✅ Feedback delivery guidelines with examples ✅ Learner response process ✅ Monitoring and quality assurance ✅ Timeline and scheduling ✅ Common mistakes and how to avoid them ✅ Policies and ethical guidelines ✅ Evaluation rubrics
Next Component: 10% Course Review Quiz (20 comprehensive questions)