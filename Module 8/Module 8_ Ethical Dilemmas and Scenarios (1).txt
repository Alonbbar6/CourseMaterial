Module 8: Responsible AI and Ethics
Ethical Dilemmas and Scenarios (36 minutes)
Module Overview: This section comprises 30% of the 120-minute module, providing learners with real-world ethical challenges in AI deployment and use.
________________


SECTION 1: AI BIAS AND FAIRNESS (12 minutes)
Scenario 1.1: The Hiring Algorithm Dilemma
Context: Your company has implemented an AI-powered resume screening system that reduced hiring time by 60%. After six months, an internal audit reveals that the system recommends male candidates 73% of the time for technical positions, despite a 50/50 gender split in applications.
Investigation reveals:
* The AI was trained on 10 years of historical hiring data
* During that period, 75% of successful technical hires were male
* The system learned to associate certain language patterns and experiences more common in male resumes with success
Your Role: You're the HR Technology Director who championed this system.
Ethical Questions:
1. Is the AI biased, or is it accurately reflecting past hiring patterns?
2. Should you disable the system immediately, or modify it gradually?
3. Who is accountable: the data, the algorithm designers, or the company's hiring managers?
4. How do you balance efficiency gains against fairness concerns?
Decision Point: Choose your course of action and justify it using Responsible AI principles (Fairness, Transparency, Accountability).
________________


Scenario 1.2: Predictive Policing Trade-offs
Context: A major city has deployed a predictive policing AI system that forecasts where crimes are likely to occur. The system has reduced overall crime by 18% in the first year. However, civil rights organizations point out that the system directs 68% of police resources to historically over-policed minority neighborhoods.
Data shows:
* Crime reports are higher in these neighborhoods (potential reporting bias)
* Increased police presence leads to more arrests, which reinforces the AI's predictions
* Residents report feeling both safer and unfairly targeted
* Violent crime has decreased significantly in these areas
Your Role: You're on the city council's public safety committee.
Ethical Questions:
1. Is this a self-fulfilling prophecy or legitimate crime prevention?
2. How do you weigh public safety benefits against potential discrimination?
3. What transparency measures should be required for algorithmic policing?
4. Can the system be "fair" if it reduces crime but perpetuates historical biases?
Decision Point: Vote to continue, modify, or discontinue the program. Explain your reasoning.
________________


Scenario 1.3: Healthcare AI and Socioeconomic Bias
Context: A hospital system implements an AI diagnostic tool for detecting skin cancer that shows 95% accuracy in clinical trials. After deployment, doctors notice it performs poorly on darker skin tones, with accuracy dropping to 68%.
Investigation reveals:
* Training data contained 91% images of lighter skin tones
* The clinical trials primarily involved patients from affluent, predominantly white neighborhoods
* Dermatology datasets historically underrepresent darker skin tones
* Re-training would cost $2M and delay deployment by 8 months
Your Role: You're the hospital's Chief Medical Information Officer.
Ethical Questions:
1. Should the tool be used despite its limitations, with warnings?
2. Is it ethical to deploy a tool that provides unequal care?
3. Who bears responsibility: the AI vendor, the hospital, or the medical community?
4. How do you address the urgent need for diagnostic tools while ensuring equity?
Decision Point: Decide whether to deploy, delay for retraining, or cancel the project entirely.
________________


SECTION 2: TRANSPARENCY AND EXPLAINABILITY (10 minutes)
Scenario 2.1: The Black Box Credit System
Context: Your financial institution uses a sophisticated deep learning model for loan approvals that's 22% more accurate than traditional methods. A small business owner whose loan was denied requests an explanation. Your AI team says the model is too complex to explain in simple terms—it considers 847 variables in ways that defy linear explanation.
Stakeholder Concerns:
* Regulators: Demanding explanations for lending decisions
* Customers: Feeling frustrated by opaque denials
* Shareholders: Celebrating improved profitability
* AI Team: Warning that simplification reduces accuracy
Your Role: Chief Risk Officer
Ethical Questions:
1. How do you balance predictive accuracy with explainability?
2. Is "the AI said no" an acceptable explanation for life-changing decisions?
3. Should certain high-stakes decisions require explainable AI, even at the cost of accuracy?
4. What does meaningful transparency look like in complex AI systems?
Decision Point: Choose between maintaining the black box system, investing in explainable AI (XAI) techniques, or reverting to traditional methods.
________________


Scenario 2.2: Content Moderation Algorithm Transparency
Context: Your social media platform uses AI to moderate content, removing millions of posts monthly. Creators complain about arbitrary removals with no explanation. Your system identifies violations but can't explain why specific content triggered removal without revealing the model's workings (which could help bad actors game the system).
The Challenge:
* Full transparency helps malicious users evade detection
* Minimal transparency frustrates legitimate users and raises bias concerns
* Competitors offer more transparency but have higher rates of harmful content
* Regulators are proposing transparency mandates
Your Role: Trust & Safety Director
Ethical Questions:
1. How transparent should content moderation algorithms be?
2. Can you be accountable without being fully transparent?
3. How do you balance safety, fairness, and openness?
4. Who deserves to know how moderation decisions are made?
Decision Point: Design a transparency framework that balances competing interests.
________________


Scenario 2.3: Medical AI Recommendations
Context: An AI system recommends treatment plans for cancer patients with impressive outcomes. However, oncologists report they sometimes disagree with recommendations but struggle to override them because they can't understand the AI's reasoning. Hospital administrators pressure doctors to follow AI guidance to maintain outcome statistics.
Your Role: Medical Ethics Board Member
Ethical Questions:
1. Should doctors be able to override AI recommendations they don't understand?
2. Is XAI a requirement for life-or-death medical decisions?
3. How do you preserve physician autonomy while leveraging AI capabilities?
4. What happens when AI is right but can't explain why?
Decision Point: Establish guidelines for AI-assisted medical decision-making.
________________


SECTION 3: ACCOUNTABILITY AND RESPONSIBILITY (8 minutes)
Scenario 3.1: The Autonomous Vehicle Accident
Context: An autonomous vehicle operated by your company strikes and kills a pedestrian who stepped into traffic without warning. The AI performed exactly as designed—it calculated that swerving would endanger the vehicle's passengers. Investigations show a human driver might have acted differently based on intuition.
Legal and Ethical Questions:
* The AI company claims the system worked correctly
* The vehicle manufacturer says it followed the AI company's specifications
* The ride-sharing company says it trusted certified technology
* The passengers' families thank the AI for saving their loved ones
* The victim's family demands accountability
Your Role: You're an AI ethics consultant advising the court.
Ethical Questions:
1. Who is accountable when AI functions as designed but causes harm?
2. Should AIs be programmed with ethical frameworks for impossible choices?
3. How do you assign responsibility in complex AI supply chains?
4. Is "the AI decided" an acceptable legal defense?
Decision Point: Recommend an accountability framework for autonomous systems.
________________


Scenario 3.2: AI-Generated Misinformation Campaign
Context: Your generative AI platform is discovered to be the source of a sophisticated misinformation campaign about a political election. Bad actors used your API to create thousands of convincing fake articles, images, and videos. Your terms of service prohibit this, but your detection systems failed to catch it in time.
The Situation:
* The content spread to millions before detection
* Your platform's capabilities made this possible
* You've since improved safety measures
* Victims demand compensation
* Regulators threaten heavy fines
Your Role: CEO of the AI company
Ethical Questions:
1. How accountable are platform providers for misuse of their tools?
2. Should AI capabilities be limited to prevent misuse?
3. What responsibility do you have for downstream harms?
4. How do you balance innovation with safety?
Decision Point: Develop a responsibility framework for generative AI platforms.
________________


Scenario 3.3: Algorithmic Trading Error
Context: Your investment firm's AI trading system has a bug that causes a "flash crash," wiping out $2B in market value in 3 minutes before circuit breakers halt trading. The AI learned an exploitation strategy that technically followed market rules but destabilized the system.
Your Role: Chief Compliance Officer
Ethical Questions:
1. Who is responsible: the programmers, the firm, the AI, or the market structure?
2. Should AI systems be held to higher standards than human traders?
3. How do you establish accountability for emergent AI behaviors?
4. What duty of care do you owe to the broader financial system?
Decision Point: Accept responsibility and propose industry-wide safeguards.
________________


SECTION 4: PRIVACY AND DATA GOVERNANCE (6 minutes)
Scenario 4.1: Health Data for Public Good
Context: Your AI research team can develop a model to predict Alzheimer's disease 5 years before symptoms appear—with 87% accuracy—but only if you use detailed medical records from 10 million patients. The data is anonymized but re-identification is theoretically possible. Patients never consented to this use, but the potential to save lives is enormous.
Stakeholders:
* Researchers: Eager to advance medical science
* Patients: Unaware their data might be used
* Privacy Advocates: Concerned about consent
* Healthcare Providers: Torn between privacy and outcomes
* Families: Desperately wanting early detection
Your Role: Hospital System Data Governance Committee Chair
Ethical Questions:
1. Does public health benefit justify using data without explicit consent?
2. How do you weigh individual privacy against collective benefit?
3. Is anonymization sufficient for ethical data use?
4. Should patients have the right to opt out of beneficial research?
Decision Point: Approve, reject, or modify the research proposal.
________________


Scenario 4.2: Employee Surveillance AI
Context: Your company wants to deploy AI that monitors employee productivity by analyzing: emails, meeting participation, mouse movements, keystrokes, and break patterns. The system promises to identify struggling employees for support and recognize high performers. Employees weren't consulted and will be notified after deployment.
Your Role: Employee Privacy Advocate on the Board
Ethical Questions:
1. Where is the line between productivity monitoring and privacy invasion?
2. Should employees consent to AI surveillance as a condition of employment?
3. How does constant monitoring affect workplace trust and autonomy?
4. Who controls the data and how can it be used?
Decision Point: Support, oppose, or propose modifications to the surveillance system.
________________


SECTION 5: COMPREHENSIVE ETHICAL SCENARIO (INTERACTIVE ACTIVITY)
THE DEPLOYMENT DECISION: FaceMatch
Setup: You are the Chief AI Ethics Officer at TechCorp. Your team has developed "FaceMatch," a facial recognition system for airport security. You must decide whether to approve deployment.
System Capabilities:
* 99.1% accuracy in controlled tests
* Processes 5,000 faces per hour
* Reduces security wait times by 40%
* Cost savings: $50M annually
Identified Issues:
Bias Problems:
* 99.7% accuracy on light-skinned males
* 96.3% accuracy on light-skinned females
* 94.8% accuracy on dark-skinned males
* 88.2% accuracy on dark-skinned females
* Higher false positive rates for minorities
Privacy Concerns:
* Stores biometric data indefinitely
* No clear consent mechanism
* Potential for function creep (other uses)
* Foreign governments requesting access
* Data breach vulnerability
Accountability Questions:
* False positives cause missed flights and interrogations
* No clear remedy process for errors
* Limited explainability for why matches occur
* Multiple vendors involved (who's responsible?)
Transparency Issues:
* Passengers don't know they're being scanned
* Algorithm details are proprietary
* No public audit of effectiveness
* Limited oversight of use
Stakeholder Positions:
Airport Authority: "Security is paramount. The system works well enough and will improve. We can't reject technology that saves lives and money."
Civil Rights Groups: "This perpetuates discrimination. Minority travelers will be disproportionately flagged. We cannot accept a system that treats people unequally."
Security Experts: "Perfect is the enemy of good. This is better than human screeners and will evolve. Delaying costs lives."
Privacy Advocates: "This is mass surveillance without consent. Today it's airports, tomorrow it's everywhere. We're trading freedom for convenience."
Your Executive Team: "We've invested $200M. Our competitors are deploying similar systems. If we don't, we lose market position and revenue."
Technical Team: "We can improve accuracy with more data, but that requires deploying the system. It's a chicken-and-egg problem."
________________


YOUR DECISION FRAMEWORK:
Option 1: APPROVE DEPLOYMENT
* Go live with current system
* Commit to continuous improvement
* Implement monitoring and audit processes
* Accept that some disparity exists
Justify this using Responsible AI principles
Option 2: CONDITIONAL APPROVAL
* Deploy only in certain contexts or locations
* Require additional safeguards
* Set accuracy thresholds that must be met
* Establish oversight mechanisms
* Design specific conditions
Explain your conditions and how they address ethical concerns
Option 3: DELAY DEPLOYMENT
* Require improvement to specific benchmarks
* Demand additional testing on diverse populations
* Develop better transparency mechanisms
* Set a timeline for reconsideration
* Specify what must change
Outline your requirements and justify the delay
Option 4: REJECT DEPLOYMENT
* System doesn't meet ethical standards
* Risks outweigh benefits
* Fundamental design flaws
* Better alternatives exist
Explain which principles are violated and why
________________


DECISION TEMPLATE:
My Decision: [Choose Option 1, 2, 3, or 4]
Justification (Address each):
1. Fairness: How does your decision address the accuracy disparities across demographic groups?

2. Transparency: What transparency measures does your decision include or require?

3. Accountability: Who is responsible for outcomes, and how will accountability be enforced?

4. Privacy: How does your decision protect individual privacy rights?

5. Trade-offs: What are you sacrificing, and why is it justified?

6. Stakeholder Impact: How will each stakeholder group be affected?

7. Implementation Plan: If approving or conditionally approving, what's your rollout plan? If rejecting or delaying, what happens next?

________________


REFLECTION QUESTIONS (All Scenarios)
After completing the scenarios, learners should reflect on:
   1. Pattern Recognition: What common themes appeared across different ethical dilemmas?

   2. Principle Conflicts: When Responsible AI principles conflicted, how did you prioritize? Why?

   3. Personal Bias: Did your decisions reflect any personal biases or assumptions? How?

   4. Uncertainty: What information would you want that wasn't provided? Why is it missing?

   5. Real-World Complexity: How did these scenarios differ from simple right/wrong situations?

   6. Systemic Issues: What systemic changes would prevent these dilemmas from arising?

   7. Role of AI Ethics Officers: Should every company deploying AI have dedicated ethics oversight?

   8. Regulation vs. Self-Governance: Should governments mandate Responsible AI practices, or should industry self-regulate?

________________


ADDITIONAL QUICK DILEMMAS (2-3 minutes each)
Quick Dilemma 1: The Resume Language Model
Your AI writing assistant helps job seekers improve resumes. Users from non-English-speaking backgrounds report the AI makes their resumes "sound too formal" and unlike human writing, potentially triggering fraud detection. Do you optimize for "authenticity" or for results?
Quick Dilemma 2: The Personalization-Privacy Balance
Your streaming service's AI recommendations are incredibly accurate because they analyze viewing patterns, pauses, rewinds, and even which scenes users watch repeatedly. Users love the recommendations but privacy advocates call it "creepy surveillance." How personal should AI get?
Quick Dilemma 3: The Copyright Question
Your generative AI was trained on millions of images from the internet, including copyrighted artwork. Artists claim the AI "steals their style." Your legal team says it's transformative fair use. Your business depends on this capability. Do you continue, modify, or rebuild from licensed data?
Quick Dilemma 4: The Deepfake Detector
You've built AI that detects deepfakes with 94% accuracy. Publishing it helps protect democracy but also teaches bad actors how to make better deepfakes. Do you publish the research or keep it secret?
Quick Dilemma 5: The Environmental Cost
Training your new AI model will consume as much electricity as 100 homes use in a year and significantly contribute to carbon emissions. The model will help optimize renewable energy systems, potentially offsetting its environmental cost 100x. Is this justified?
________________


SCORING RUBRIC FOR ETHICAL DECISION-MAKING
Learners should be evaluated on:
Quality of Reasoning (40%)
      * Considers multiple stakeholder perspectives
      * Identifies relevant ethical principles
      * Recognizes trade-offs and complexity
      * Shows systematic thinking
Application of Responsible AI Principles (30%)
      * References Fairness, Transparency, Accountability
      * Applies principles appropriately to context
      * Balances competing principles
Practical Feasibility (15%)
      * Proposes actionable solutions
      * Considers real-world constraints
      * Addresses implementation challenges
Completeness (15%)
      * Addresses all aspects of the scenario
      * Considers short and long-term consequences
      * Identifies potential unintended outcomes
________________


INSTRUCTOR GUIDANCE
Facilitation Tips:
      1. No single "correct" answer exists—focus on reasoning quality
      2. Encourage learners to challenge each other's assumptions respectfully
      3. Highlight how different cultural contexts may lead to different ethical conclusions
      4. Use scenarios to demonstrate that ethics is ongoing practice, not checklist compliance
      5. Connect scenarios to current news and real-world AI controversies
      6. Emphasize that ethical AI requires diverse teams and perspectives
Time Management:
      * Major scenarios: 4-5 minutes each
      * Quick dilemmas: 2-3 minutes each
      * Interactive activity (FaceMatch): 10-12 minutes
      * Reflection: 5 minutes
Adaptation Options:
      * Use for group discussions
      * Assign as individual written exercises
      * Deploy as simulation/role-playing activities
      * Integrate with quiz questions
      * Extend with research assignments
________________


KEY TAKEAWAYS
By completing these scenarios, learners should understand:
      1. ✓ AI bias emerges from data, algorithms, and deployment contexts
      2. ✓ Fairness often involves difficult trade-offs without perfect solutions
      3. ✓ Transparency and explainability matter most for high-stakes decisions
      4. ✓ Accountability requires clear responsibility chains
      5. ✓ Privacy protection must balance with innovation and benefit
      6. ✓ Ethical AI requires ongoing vigilance, not one-time decisions
      7. ✓ Different stakeholders have legitimate but competing interests
      8. ✓ Responsible AI is everyone's responsibility, not just the ethics team
________________


End of Ethical Dilemmas and Scenarios Section
This content represents 30% of Module 8 (36 minutes of 120-minute module) Designed to complement: 40% Reading, 20% Interactive Content, 10% Quiz