MODULE 5: NLP APPLICATIONS - EXAMPLES & VISUALIZATIONS
Real-World Applications and Visual Demonstrations (36 minutes)
MTW AI Platform | 30% of Module Content
________________


CONTENT STRUCTURE
Example Set 1: Tokenization in Action (6 min)
Example Set 2: Sentiment Analysis (8 min)
Example Set 3: Named Entity Recognition (6 min)
Example Set 4: Word Embeddings Visualized (8 min)
Example Set 5: Real-World Applications (8 min)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL: 36 minutes


________________


EXAMPLE SET 1: TOKENIZATION IN ACTION
Time: 6 minutes | Concept: Breaking Text into Processable Units
Visual 1: Simple Tokenization Process
INPUT TEXT:
"I love learning AI! It's amazing."


STEP 1: Word Tokenization
["I", "love", "learning", "AI", "!", "It's", "amazing", "."]


STEP 2: Sentence Tokenization
["I love learning AI!", "It's amazing."]


STEP 3: Subword Tokenization (BPE - Byte Pair Encoding)
["I", "love", "learn", "##ing", "AI", "!", "It", "'s", "amaz", "##ing", "."]


Why Multiple Approaches?
Word Tokenization:
* Simple, intuitive
* Problem: Unknown words ("superduper") â†’ can't process
Subword Tokenization:
* Handles rare/new words
* "superduper" â†’ ["super", "##dup", "##er"]
* Can understand from parts
Example 2: Tokenization Challenges
SENTENCE: "We're meeting at 3:30pm at joe@email.com's office."


CHALLENGE 1: Contractions
"We're" â†’ ["We", "'re"] or ["We're"]?


CHALLENGE 2: Timestamps
"3:30pm" â†’ ["3:30pm"] or ["3", ":", "30", "pm"]?


CHALLENGE 3: Emails
"joe@email.com" â†’ Keep together or split?


CHALLENGE 4: Possessives
"joe@email.com's" â†’ How to handle?


NLP SOLUTION: Context-aware tokenization
âœ“ Keep "3:30pm" together (time entity)
âœ“ Keep "joe@email.com" together (email entity)
âœ“ Split "We're" â†’ ["We", "are"] for understanding


Visualization: Tokenization Comparison
TEXT: "Don't underestimate AI!"


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ METHOD          â”‚ OUTPUT                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Word-based      â”‚ ["Don't", "underestimate", "AI", "!"] â”‚
â”‚ Character-based â”‚ ["D","o","n","'","t"," ","u","n",...] â”‚
â”‚ Subword (BPE)   â”‚ ["Don", "'t", "under", "##estimate",  â”‚
â”‚                 â”‚  "AI", "!"]                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


TRADEOFFS:
Word: Simple but can't handle unknown words
Character: Handles everything but loses meaning
Subword: Best of both âœ“


________________


EXAMPLE SET 2: SENTIMENT ANALYSIS
Time: 8 minutes | Concept: Understanding Opinion and Emotion
Example 1: Basic Sentiment Classification
REVIEW 1: "This product is amazing! Best purchase ever!"
SENTIMENT: Positive (95% confidence)
KEYWORDS: "amazing", "best", "ever" (all positive indicators)


REVIEW 2: "Terrible quality. Waste of money."
SENTIMENT: Negative (92% confidence)
KEYWORDS: "terrible", "waste" (negative indicators)


REVIEW 3: "It's okay. Does what it says."
SENTIMENT: Neutral (78% confidence)
KEYWORDS: "okay" (neutral indicator)


Visualization: Sentiment Score Scale
NEGATIVE â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ POSITIVE
   -1         -0.5        0        0.5         1
    â—                              â—           â—
"Terrible"                    "Okay"   "Amazing!"


COMPOUND SCORES:
-1.0 to -0.6: Very Negative
-0.6 to -0.2: Negative
-0.2 to +0.2: Neutral
+0.2 to +0.6: Positive
+0.6 to +1.0: Very Positive


Example 2: Tricky Cases (Sarcasm, Negation)
CASE 1: Negation
âŒ "not bad" â†’ Naive model sees "bad" â†’ classifies Negative
âœ“ Advanced model understands "not" inverts meaning â†’ Positive


CASE 2: Sarcasm
"Oh great, another software update that broke everything."
âŒ Naive model sees "great" â†’ classifies Positive
âœ“ Context-aware model detects sarcasm â†’ Negative


CASE 3: Mixed Sentiment
"Great product but terrible customer service."
ASPECT-BASED SENTIMENT:
- Product: Positive
- Customer Service: Negative
- Overall: Mixed/Neutral


Real Example: Product Review Analysis
INPUT: Amazon Product Review
"I bought this laptop for my daughter. The screen is beautiful and 
battery lasts all day. However, it's pretty heavy for carrying to 
school, and setup was confusing. Customer service helped though."


ASPECT ANALYSIS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Aspect           â”‚ Sentiment  â”‚ Key Phrase         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Display          â”‚ Positive   â”‚ "screen beautiful" â”‚
â”‚ Battery          â”‚ Positive   â”‚ "lasts all day"    â”‚
â”‚ Portability      â”‚ Negative   â”‚ "pretty heavy"     â”‚
â”‚ Setup            â”‚ Negative   â”‚ "confusing"        â”‚
â”‚ Customer Service â”‚ Positive   â”‚ "helped"           â”‚
â”‚ OVERALL          â”‚ Mixed      â”‚ 3 pos, 2 neg       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Visualization: Sentiment Over Time
SOCIAL MEDIA MONITORING: Brand Sentiment During Product Launch


Sentiment
  +1 â”‚         â•±â•²
     â”‚        â•±  â•²
   0 â”‚â”€â”€â”€â”€â”€â”€â”€â•±    â•²â”€â”€â”€â”€â”€â”€â”€â”€â”€
     â”‚      â•±      â•²
  -1 â”‚     â•±        â•²
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Time
     Launch  Day 2  Day 5  Day 7
     
Day 1: Launch excitement (Positive spike)
Day 2-3: Peak enthusiasm
Day 5: Issue discovered (Negative dip)
Day 7: Company fixes issue (Recovering)


________________


EXAMPLE SET 3: NAMED ENTITY RECOGNITION (NER)
Time: 6 minutes | Concept: Identifying Important Entities
Example 1: Basic NER Tagging
SENTENCE: "Apple CEO Tim Cook announced the new iPhone in California."


ENTITY EXTRACTION:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Entity       â”‚ Type        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Apple        â”‚ ORGANIZATIONâ”‚
â”‚ Tim Cook     â”‚ PERSON      â”‚
â”‚ iPhone       â”‚ PRODUCT     â”‚
â”‚ California   â”‚ LOCATION    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Visualization: NER Highlighting
TEXT WITH NER ANNOTATIONS:


"[PERSON: Elon Musk] founded [ORG: SpaceX] in [DATE: 2002] in 
[LOC: Hawthorne, California]. The company launched [PRODUCT: Falcon 9] 
to deliver cargo worth [MONEY: $1.6 billion]."


COLOR CODING:
ğŸŸ¦ PERSON (blue)
ğŸŸ© ORGANIZATION (green)
ğŸŸ¨ LOCATION (yellow)
ğŸŸ§ PRODUCT (orange)
ğŸŸ¥ DATE (red)
ğŸŸª MONEY (purple)


Example 2: Real-World Application - News Article Analysis
INPUT: News Headline
"Microsoft acquires OpenAI stake for $10B, names Sam Altman to board"


NER OUTPUT:
Entity          Type         Context
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Microsoft       ORG          Acquirer
OpenAI          ORG          Target company
$10B            MONEY        Deal value
Sam Altman      PERSON       Appointee
board           ROLE         Position


USE CASES:
âœ“ Financial analysis (track deals, companies, amounts)
âœ“ Relationship extraction (Microsoft â†’ invests in â†’ OpenAI)
âœ“ Timeline creation (events, dates, people involved)


Example 3: Ambiguity Resolution
AMBIGUOUS SENTENCE:
"I love the new Apple watch I bought in Washington last May."


CHALLENGE: Multiple interpretations
- "Apple" â†’ Fruit or Company?
- "Washington" â†’ State or DC?
- "May" â†’ Month or person named May?


NER CONTEXT RESOLUTION:
"I love the new [PRODUCT: Apple watch] I bought in 
[LOCATION: Washington] last [DATE: May]."


CLUES:
âœ“ "watch" after "Apple" â†’ technology product
âœ“ "bought in" suggests location
âœ“ "last" suggests time period


________________


EXAMPLE SET 4: WORD EMBEDDINGS VISUALIZED
Time: 8 minutes | Concept: Words as Mathematical Vectors
Visualization 1: Word Vectors in 2D Space
CONCEPT: Similar words cluster together in vector space


        Programming
            â—
        Python â”‚
            â—  â”‚  â— Java
               â”‚
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ (axis 1: technology)
               â”‚
         Dog â— â”‚ â— Cat
            Animal
               â”‚
            (axis 2: living things)


DISTANCE = SIMILARITY
- Python â†â†’ Java: Close (both programming languages)
- Dog â†â†’ Cat: Close (both animals)
- Python â†â†’ Dog: Far (different concepts)


Example 1: Word Analogy (Vector Arithmetic)
FAMOUS WORD2VEC EXAMPLE:
King - Man + Woman = Queen


MATHEMATICAL REPRESENTATION:
Vector(King) - Vector(Man) + Vector(Woman) â‰ˆ Vector(Queen)


WHY IT WORKS:
King = [royalty, male, power, ...]
Man = [male, adult, human, ...]
Woman = [female, adult, human, ...]


King - Man = [royalty, power, ...] (remove "male")
+ Woman = [royalty, power, female, ...] = Queen âœ“


MORE EXAMPLES:
Paris - France + Italy = Rome
walked - walking + swimming = swam
bigger - big + small = smaller


Visualization 2: Embedding Dimensions Capture Meaning
WORD: "King"


Dimension 1 (Gender):        â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Male
Dimension 2 (Age):           â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Adult
Dimension 3 (Power):         â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ High Authority
Dimension 4 (Wealth):        â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Rich
Dimension 5 (Occupation):    â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Royalty
...
Dimension 300                [more subtle patterns]


WORD: "Queen"
Dimension 1 (Gender):        â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â— Female
Dimension 2 (Age):           â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Adult
Dimension 3 (Power):         â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ High Authority
Dimension 4 (Wealth):        â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Rich
Dimension 5 (Occupation):    â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Royalty


SIMILAR on most dimensions (royalty, power, wealth)
DIFFERENT on gender dimension


Example 2: Real Application - Document Search
USER QUERY: "machine learning algorithms"


TRADITIONAL KEYWORD SEARCH:
Finds only documents containing exact words "machine", "learning", "algorithms"
Misses: "neural networks", "deep learning models", "AI techniques"


EMBEDDING-BASED SEARCH:
Convert query to vector: [0.2, 0.8, 0.3, ...]
Find documents with similar vectors


RESULTS INCLUDE:
âœ“ "Deep learning models for classification"
âœ“ "Introduction to neural networks"
âœ“ "AI techniques for data analysis"


WHY? Embeddings understand semantic similarity:
"machine learning" â‰ˆ "deep learning" â‰ˆ "neural networks"


Visualization 3: Embedding Space Clustering
WORD EMBEDDING CLUSTERS (simplified 2D projection)


    Technology Cluster          Animal Cluster
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ â— computer   â”‚           â”‚ â— dog        â”‚
    â”‚ â— laptop     â”‚           â”‚ â— cat        â”‚
    â”‚ â— software   â”‚           â”‚ â— bird       â”‚
    â”‚ â— code       â”‚           â”‚ â— fish       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    Food Cluster               Color Cluster
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ â— pizza      â”‚           â”‚ â— red        â”‚
    â”‚ â— burger     â”‚           â”‚ â— blue       â”‚
    â”‚ â— salad      â”‚           â”‚ â— green      â”‚
    â”‚ â— pasta      â”‚           â”‚ â— yellow     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


PRINCIPLE: Words used in similar contexts â†’ similar vectors


________________


EXAMPLE SET 5: REAL-WORLD NLP APPLICATIONS
Time: 8 minutes | Concept: NLP in Daily Life
Application 1: Customer Service Chatbots
EXAMPLE: E-commerce Support Bot


USER: "Where's my order? It was supposed to arrive yesterday."


NLP PROCESSING:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Intent Classification                        â”‚
â”‚    â†’ Intent: order_tracking (98% confidence)    â”‚
â”‚                                                  â”‚
â”‚ 2. Entity Extraction                           â”‚
â”‚    â†’ Time: "yesterday" (expected delivery)      â”‚
â”‚                                                  â”‚
â”‚ 3. Sentiment Analysis                          â”‚
â”‚    â†’ Sentiment: Frustrated (keywords: "supposedâ”‚
â”‚       to arrive")                               â”‚
â”‚                                                  â”‚
â”‚ 4. Generate Response                           â”‚
â”‚    â†’ Action: Look up order, provide tracking    â”‚
â”‚    â†’ Tone: Apologetic, helpful                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


BOT RESPONSE:
"I understand your concern. Let me check your order status right away. 
I see it's currently in transit and should arrive by end of day today. 
I apologize for the delay. Would you like me to provide the tracking 
number?"


NLP CAPABILITIES USED:
âœ“ Intent detection (what user wants)
âœ“ NER (time reference extraction)
âœ“ Sentiment awareness (adjust tone)
âœ“ Natural response generation


Application 2: Social Media Monitoring
BRAND: Miami Coffee Shop "CafÃ© Cubano"


MONITORING DASHBOARD:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ REAL-TIME SENTIMENT TRACKING                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Tweet 1: "Best cortadito in Miami! ğŸ˜"         â”‚
â”‚ Sentiment: Positive | Engagement: 45 likes     â”‚
â”‚ Action: Retweet + Thank customer               â”‚
â”‚                                                 â”‚
â”‚ Tweet 2: "Waited 20 minutes for coffee..."     â”‚
â”‚ Sentiment: Negative | Urgency: High            â”‚
â”‚ Action: Respond immediately, offer discount    â”‚
â”‚                                                 â”‚
â”‚ Tweet 3: "They have vegan options now"         â”‚
â”‚ Sentiment: Neutral-Positive | Info: Product    â”‚
â”‚ Action: Amplify, share to vegan community      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


INSIGHTS GENERATED:
â€¢ Average sentiment: +0.7 (Positive)
â€¢ Trending topics: "cortadito", "vegan options"
â€¢ Issue detected: Wait times (3 complaints this week)
â€¢ Recommendation: Address staffing during peak hours


Application 3: Automatic Translation
MULTI-LANGUAGE CUSTOMER SUPPORT


INPUT (Spanish): "Necesito ayuda con mi pedido"


NLP PIPELINE:
1. Language Detection â†’ Spanish (99% confidence)
2. Translation to English â†’ "I need help with my order"
3. Intent Classification â†’ customer_support/order_help
4. Generate English Response
5. Translate Response to Spanish


OUTPUT (Spanish): "Por supuesto, con gusto te ayudo. Â¿CuÃ¡l es tu 
nÃºmero de pedido?"


CHALLENGES NLP SOLVES:
âœ“ Idiomatic expressions ("con gusto" not literal "with pleasure")
âœ“ Cultural context (formal "usted" vs informal "tÃº")
âœ“ Domain-specific terms ("pedido" = order in e-commerce context)


Application 4: Email Auto-Categorization
INBOX MANAGEMENT SYSTEM


EMAIL 1:
Subject: "Invoice #12345 - Payment Due"
Content: "Please find attached invoice..."
NLP CLASSIFICATION:
â†’ Category: Finance/Invoice
â†’ Priority: Medium
â†’ Action: File in "Invoices" folder
â†’ Reminder: Set payment due date


EMAIL 2:
Subject: "URGENT: Server Down"
Content: "Production server not responding..."
NLP CLASSIFICATION:
â†’ Category: Technical/Critical
â†’ Priority: URGENT
â†’ Action: Alert on-call engineer
â†’ Sentiment: Emergency (detected from "URGENT", "Down")


EMAIL 3:
Subject: "Coffee next week?"
Content: "Hey, want to grab coffee Tuesday?"
NLP CLASSIFICATION:
â†’ Category: Personal/Social
â†’ Priority: Low
â†’ Action: Suggest reply, add to calendar
â†’ Entity: Date extracted (Tuesday)


Application 5: Content Moderation
SOCIAL PLATFORM MODERATION


POST 1: "I hate Mondays lol"
ANALYSIS:
- Sentiment: Negative (casual)
- Toxicity: Low (0.1/1.0)
- Action: Approve âœ“


POST 2: [Offensive content example - implied not shown]
ANALYSIS:
- Toxicity: High (0.9/1.0)
- Categories: Harassment, Hate Speech
- Action: Remove + Warn user âœ—


POST 3: "Check out this amazing deal! [spam link]"
ANALYSIS:
- Spam probability: 95%
- Pattern: Known spam template
- Action: Mark as spam âœ—


NLP TECHNIQUES:
âœ“ Toxicity detection (identify harmful language)
âœ“ Spam classification (pattern recognition)
âœ“ Context understanding (sarcasm, idioms)
âœ“ Multi-language support (global platforms)


________________


CONNECTING EXAMPLES TO CONCEPTS
Summary Table: NLP Techniques â†’ Applications
Technique
	Example Application
	Business Value
	Tokenization
	Search engines
	Better query understanding
	Sentiment Analysis
	Brand monitoring
	Track reputation, respond to issues
	NER
	News aggregation
	Extract key info (people, places, events)
	Word Embeddings
	Recommendation systems
	Find similar content/products
	Intent Classification
	Chatbots
	Route queries to right department
	Translation
	Global commerce
	Reach international customers
	Key Takeaway Visualization
NLP TRANSFORMS UNSTRUCTURED TEXT â†’ STRUCTURED INSIGHTS


INPUT                    NLP PROCESSING              OUTPUT
â”€â”€â”€â”€â”€â”€                   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€
Customer reviews    â†’    Sentiment Analysis    â†’    Satisfaction score
Support tickets     â†’    Intent + NER          â†’    Auto-routing
Social media posts  â†’    Topic modeling        â†’    Trending themes
Emails              â†’    Classification        â†’    Priority inbox
Documents           â†’    Entity extraction     â†’    Knowledge graph
Multilingual text   â†’    Translation           â†’    Global reach


________________


PRACTICE EXERCISE: APPLY YOUR UNDERSTANDING
Given this customer review: "The Miami location has great service but the parking situation is terrible. Been coming here since 2020 and it keeps getting worse."
Identify:
1. Sentiment per aspect (service vs. parking)
2. Named entities (location, date)
3. Key tokens that carry meaning
4. How word embeddings would help find similar reviews
Answers:
1. Service: Positive ("great") | Parking: Negative ("terrible", "getting worse")
2. Location: Miami | Date: 2020
3. Key tokens: "great", "terrible", "worse" (sentiment carriers)
4. Embeddings would cluster with reviews mentioning "parking issues", "service quality"
________________


EXAMPLES & VISUALIZATIONS COMPLETE âœ“
Total Time: 36 minutes
Format: 5 example sets with visuals
Coverage: Tokenization, Sentiment, NER, Embeddings, Applications
Integration: Prepares for hands-on lab (sentiment analysis)
Next: Module 5 Hands-On Lab (20% = 24 minutes)